{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Documents\\VS Code Kodlari\\yeni_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ASUS\\Documents\\VS Code Kodlari\\yeni_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Temizlemek istediÄŸiniz tweet metinlerini liste olarak ekleyin\n",
    "tweets = [\n",
    "    \"Check out our website! https://example.com #innovation @company\",\n",
    "    \"Wow! This is amazing #AI @user123 ðŸ˜Š\",\n",
    "    \"Visit us at https://anotherlink.com #exciting @anotheruser\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temizlik fonksiyonu\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)  # Linkleri kaldÄ±rma\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)     # KullanÄ±cÄ± adlarÄ±nÄ± kaldÄ±rma\n",
    "    tweet = re.sub(r'#\\w+', '', tweet)     # Hashtagleri kaldÄ±rma\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)  # Ã–zel karakterleri kaldÄ±rma\n",
    "    tweet = tweet.strip()                  # Fazla boÅŸluklarÄ± temizleme\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TemizlenmiÅŸ tweetleri iÃ§eren bir liste oluÅŸturun\n",
    "cleaned_tweets = [clean_tweet(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out our website\n",
      "Wow This is amazing\n",
      "Visit us at\n"
     ]
    }
   ],
   "source": [
    "# TemizlenmiÅŸ tweetleri yazdÄ±rÄ±n\n",
    "for tweet in cleaned_tweets:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check out our website\n",
      "wow this is amazing\n",
      "visit us at\n"
     ]
    }
   ],
   "source": [
    "# KÃ¼Ã§Ã¼k harfe dÃ¶nÃ¼ÅŸtÃ¼rme fonksiyonu\n",
    "def lowercase_tweet(tweet):\n",
    "    return tweet.lower()\n",
    "\n",
    "# KÃ¼Ã§Ã¼k harfe dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ tweetler\n",
    "lowercased_tweets = [lowercase_tweet(tweet) for tweet in cleaned_tweets]\n",
    "\n",
    "# Sonucu yazdÄ±rma\n",
    "for tweet in lowercased_tweets:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check website\n",
      "wow amazing\n",
      "visit us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stop words listesini indir (ilk seferde Ã§alÄ±ÅŸtÄ±rmanÄ±z gerekebilir)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Stop words setini oluÅŸtur (TÃ¼rkÃ§e stop words iÃ§in 'turkish' parametresini kullanabilirsiniz)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Stop words Ã§Ä±karma fonksiyonu\n",
    "def remove_stop_words(tweet):\n",
    "    words = tweet.split()  # Tweeti kelimelerine ayÄ±rma\n",
    "    return \" \".join([word for word in words if word not in stop_words])\n",
    "\n",
    "# Stop words Ã§Ä±karÄ±lmÄ±ÅŸ tweetler\n",
    "no_stopwords_tweets = [remove_stop_words(tweet) for tweet in lowercased_tweets]\n",
    "\n",
    "# Sonucu yazdÄ±rma\n",
    "for tweet in no_stopwords_tweets:\n",
    "    print(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Documents\\VS Code Kodlari\\yeni_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ASUS\\Documents\\VS Code Kodlari\\yeni_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: check out our website\n",
      "Sentiment: [{'label': 'LABEL_1', 'score': 0.5331453680992126}]\n",
      "Tweet: wow this is amazing\n",
      "Sentiment: [{'label': 'LABEL_1', 'score': 0.5415101647377014}]\n",
      "Tweet: visit us at\n",
      "Sentiment: [{'label': 'LABEL_1', 'score': 0.5321547389030457}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# DistilBERT modelini duygu analizi iÃ§in ayarla\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased\")\n",
    "\n",
    "# Ã–niÅŸlemden geÃ§miÅŸ tweetlerde duygu analizi yapma\n",
    "sentiments = [sentiment_analysis(tweet) for tweet in lowercased_tweets]\n",
    "\n",
    "# SonuÃ§larÄ± yazdÄ±r\n",
    "for tweet, sentiment in zip(lowercased_tweets, sentiments):\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yeni_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
